{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPANY DATA METHOD NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column names and dataframe\n",
    "\n",
    "path = 'sec_edgar_filings/917520/10-K/0000917520-20-000010.txt'\n",
    "file = open(path)\n",
    "lines = [line for line in file.readlines() if line.strip()]\n",
    "lines = [x.strip() for x in lines]\n",
    "for i in range(len(lines)):\n",
    "        try:\n",
    "            lines[i] = re.sub('\\t+','',lines[i])\n",
    "        except:\n",
    "            pass\n",
    "file.close()\n",
    "\n",
    "header_row = lines.index('</SEC-HEADER>')\n",
    "companystart = lines.index('FILER:') + 2\n",
    "lines = lines[companystart:header_row]\n",
    "col = []\n",
    "for i in lines:\n",
    "    col.append(i.split(':')[0])\n",
    "col.insert(13,'STREET 2')\n",
    "col.insert(20,'STREET 2')\n",
    "for ii in range(9):\n",
    "    col.append(col[24] + ' ' + f'{ii + 2}')\n",
    "    col.append(col[25] + ' ' + f'{ii + 2}')\n",
    "    col.append(col[26] + ' ' + f'{ii + 2}')\n",
    "\n",
    "df = pd.DataFrame(index = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that find the first/second occurence of a specified char in string\n",
    "\n",
    "def find_2nd(string, substring):\n",
    "    return string.find(substring, string.find(substring) + 1)\n",
    "def find_1st(string, substring):\n",
    "    return string.find(substring, string.find(substring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate paths and modify it to be linux/os/windows compatible\n",
    "\n",
    "paths10k = glob.glob('sec_edgar_filings/*/10-K/*.txt')\n",
    "paths8k = glob.glob('sec_edgar_filings/*/8-K/*.txt')\n",
    "pathsSC = glob.glob('sec_edgar_filings/*/SC 13G/*.txt')\n",
    "\n",
    "paths = paths10k + paths8k + pathsSC\n",
    "for i in range(len(paths)):\n",
    "    try:\n",
    "        paths[i] = paths[i].replace('\\\\','/')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select one path from each company, can be from SG 13C or 8-K or 10-K\n",
    "\n",
    "path = []\n",
    "for i in range(len(paths)):\n",
    "    det = True\n",
    "    for ii in path:\n",
    "        if paths[i][find_1st(paths[i],'/')+1 : find_2nd(paths[i],'/')] in ii:\n",
    "            det = False\n",
    "    if det:\n",
    "        path.append(paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate all to be one big df\n",
    "\n",
    "not_in_format = []\n",
    "for p in tqdm(path):\n",
    "    try:\n",
    "        file = open(p)\n",
    "        lines = [line for line in file.readlines() if line.strip()]\n",
    "        lines = [x.strip() for x in lines]\n",
    "        for i in range(len(lines)):\n",
    "                try:\n",
    "                    lines[i] = re.sub('\\t+','',lines[i])\n",
    "                except:\n",
    "                    pass\n",
    "        file.close()\n",
    "\n",
    "        try:\n",
    "            FB_row = lines.index('FILED BY:')\n",
    "        except:\n",
    "            FB_row = 9999\n",
    "        header_row = lines.index('</SEC-HEADER>')\n",
    "        if FB_row > header_row:\n",
    "            end = header_row\n",
    "        else:\n",
    "            end = FB_row\n",
    "        companystart = lines.index('COMPANY DATA:') + 1\n",
    "        lines = lines[companystart:end]\n",
    "        for i in range(len(col[:22])):\n",
    "            if lines[i].split(':')[0] != col[i]:\n",
    "                lines.insert(i,col[i] + ':')\n",
    "        \n",
    "        \n",
    "        values = []\n",
    "        for i in lines:\n",
    "            values.append(i.split(':')[1])\n",
    "        if len(values) != df.shape[0]:\n",
    "            for i in range(df.shape[0]-len(values)):\n",
    "                values.append('None')\n",
    "        if lines.index('MAIL ADDRESS:') != 18:\n",
    "            not_in_format.append(values[1])\n",
    "            continue\n",
    "        df[f'{values[1]}'] = values\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.to_csv('companydata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_in_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file(path):\n",
    "    infofile = []\n",
    "    infoFB = []\n",
    "#     tmp = []\n",
    "    for i in path:\n",
    "        x = i.split('/')[1]\n",
    "        \n",
    "        if 'SC 13G' not in i:\n",
    "            with open(i, 'r') as f:\n",
    "                info = f.readlines()\n",
    "            info = [x.rstrip('\\n') for x in info]\n",
    "            info = [x.strip('\\t') for x in info]\n",
    "            fileend = info.index('FILER:')\n",
    "            tmp = info[:fileend]\n",
    "            tmp = ['Filing_Company_CIK:' + x] + tmp\n",
    "        else:\n",
    "            with open(i, 'r') as f:\n",
    "                infoSC = f.readlines()\n",
    "            infoSC = [x.rstrip('\\n') for x in infoSC]\n",
    "            infoSC = [x.strip('\\t') for x in infoSC]\n",
    "            \n",
    "            fileend = infoSC.index('SUBJECT COMPANY:') - 1\n",
    "            tmp = infoSC[:fileend]\n",
    "            tmp = ['Filing_Company_CIK:' + x] + tmp\n",
    "            \n",
    "            FBstart = infoSC.index('FILED BY:')\n",
    "            header_row = infoSC.index('</SEC-HEADER>')\n",
    "            infoFB = infoSC[FBstart:header_row]\n",
    "#             infoFB = ['Primary_CIK:' + x] + infoFB\n",
    "            \n",
    "        infofile.append(tmp)\n",
    "    \n",
    "    return (infofile, infoFB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_trans_comp(info):\n",
    "\n",
    "# remove NAs and tabs\n",
    "    info = [x for x in info if x == x]\n",
    "\n",
    "    for i in range(len(info)):\n",
    "        try:\n",
    "            info[i] = re.sub('\\t+','',info[i])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "# distinguish duplicated keys in different files\n",
    "    for i in range(len(info)):\n",
    "        for ii in range(i+1,len(info)):\n",
    "            if (info[i].split(':')[0] + ':') in info[ii] and info[i] != '':\n",
    "                info[ii] = info[ii].split(':')[0] + '_' + str( \"%04d\" % ii) + ':' + info[ii].split(':')[1]\n",
    "\n",
    "# transform to dictionary\n",
    "    dic = {}\n",
    "    for i in info:\n",
    "        try:\n",
    "            dic.update({i.split(':')[0]:i.split(':')[1].strip('\\t')})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "# transform to the dataframe\n",
    "    df = pd.DataFrame(dic, index = [0])\n",
    "    \n",
    "# put the address together(street, city, state, zip)\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if 'ADDRESS' in col:\n",
    "            df[col] = df[col].str.cat(df.iloc[:,i+1:i+6],sep = ',')\n",
    "            df.iloc[:, i+1:i+6] = ''\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_file(info):\n",
    "# remove NAs and tabs\n",
    "    info = [x for x in info if x == x]\n",
    "\n",
    "    for i in range(len(info)):\n",
    "        try:\n",
    "            info[i] = info[i].replace('\\t','')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    for i in range(len(info)):\n",
    "        info[i] = info[i].lstrip()\n",
    "        if '>' in info[i] and ':' not in info[i]:\n",
    "            info[i] = info[i].replace('>', '>:')\n",
    "        elif '>' in info[i] and ':' in info[i]:\n",
    "            info[i] = info[i].replace(':',';')\n",
    "            info[i] = info[i].replace('>', '>:')\n",
    "              \n",
    "    \n",
    "# distinguish duplicated keys in different files\n",
    "    for i in range(len(info)):\n",
    "        for ii in range(i+1,len(info)):\n",
    "            if (info[i].split(':')[0] + ':') in info[ii] and info[i] != '':\n",
    "                info[ii] = info[ii].split(':')[0] + '_' + str( \"%04d\" % ii) + ':' + info[ii].split(':')[1]\n",
    "                \n",
    "                \n",
    "    info = [x for x in info if x != '']\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(info):\n",
    "# transform to dataframe\n",
    "    df_file = pd.DataFrame()\n",
    "    \n",
    "    dic_file = {}\n",
    "    idx = 0\n",
    "    for k in info:\n",
    "        for j in k:\n",
    "            try:\n",
    "                if j.split(':')[1] != '':\n",
    "                    dic_file.update({j.split(':')[0]:j.split(':')[1]})\n",
    "            except:\n",
    "                pass\n",
    "        tmp = pd.DataFrame(dic_file, index = [idx])\n",
    "\n",
    "        df_file = df_file.join(tmp.T,how = 'outer')\n",
    "\n",
    "        dic_file = {}\n",
    "        idx += 1\n",
    "    \n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = glob.glob('sec_edgar_filings/*')\n",
    "len(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = glob.glob('sec_edgar_filings/*/*/*.txt')\n",
    "companies[14369:14371]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = pd.DataFrame()\n",
    "companies = glob.glob('sec_edgar_filings/*')\n",
    "jump = 0\n",
    "\n",
    "# def final_function(companies):\n",
    "for company in tqdm(companies):\n",
    "    try:\n",
    "        path = glob.glob(company + '/*/*.txt')\n",
    "\n",
    "        infofile, infoFB = get_file(path)\n",
    "\n",
    "        infofile = list(map(cleaning_file, infofile))\n",
    "        infofile = trans(infofile)\n",
    "\n",
    "\n",
    "        if infoFB!= []:\n",
    "            infoFB = cleaning_trans_comp(infoFB)\n",
    "\n",
    "            new = []\n",
    "            for i in infoFB.columns:\n",
    "                new.append('FILED BY:' + i)\n",
    "            infoFB.columns = new\n",
    "\n",
    "            infoFB.index = infofile.T[infofile.T['CONFORMED SUBMISSION TYPE'] == 'SC 13G'].index\n",
    "            infofile = infofile.T.join(infoFB, how = 'left', lsuffix = 'CONFORMED SUBMISSION TYPE', rsuffix = 'FORM TYPE')\n",
    "\n",
    "            result = infofile.T.drop_duplicates()\n",
    "            df = result.T\n",
    "        else:\n",
    "            df = infofile.T.drop_duplicates()\n",
    "\n",
    "        df.index = df.index.map(lambda x: x + jump)\n",
    "        jump += len(df.T.columns)\n",
    "        datafile = datafile.join(df.T, how = 'outer')\n",
    "    except:\n",
    "        pass\n",
    "#     return datafile\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     executor.map(final_function,companies)\n",
    "datafile.to_csv('temp_filedata.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the columns    \n",
    "cols = list(datafile.T.columns)\n",
    "\n",
    "news = []\n",
    "olds = []\n",
    "for i in cols:\n",
    "    if 'FILED BY:' in i:\n",
    "        news.append(i)\n",
    "    else:\n",
    "        olds.append(i)\n",
    "cols = olds + news\n",
    "cols.insert(0,cols.pop(cols.index('CONFORMED SUBMISSION TYPE')))\n",
    "cols.insert(0,cols.pop(cols.index('Filing_Company_CIK')))\n",
    "cols.insert(cols.index('FILED BY:BUSINESS ADDRESS') ,cols.pop(cols.index('FILED BY:COMPANY CONFORMED NAME')))\n",
    "\n",
    "\n",
    "final_file = datafile.T.loc[:,cols].drop(['FILED BY:FILED BY'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file.to_csv('filedata.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "final_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPANY DATA METHOD ORIGINAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp(path):\n",
    "    infocomp = []\n",
    "    for i in path:\n",
    "        if 'SC 13G' not in i:\n",
    "            df = pd.read_fwf(i,header = None)\n",
    "            info = df[0].to_list()\n",
    "            header_row = info.index('</SEC-HEADER>')\n",
    "            companystart = info.index('FILER:') + 3\n",
    "            infocomp += info[companystart:header_row]\n",
    "        else:\n",
    "            df = pd.read_fwf(i,header = None)\n",
    "            infoSC = df[0].to_list() \n",
    "            companystart = infoSC.index('SUBJECT COMPANY:') + 3\n",
    "            companyend = infoSC.index('FILED BY:') - 1\n",
    "            infocomp += infoSC[companystart:companyend]\n",
    "            \n",
    "    return infocomp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_comp(info):\n",
    "\n",
    "# remove NAs and tabs\n",
    "    info = [x for x in info if x == x]\n",
    "\n",
    "    for i in range(len(info)):\n",
    "        try:\n",
    "            info[i] = re.sub('\\t+','',info[i])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "# distinguish duplicated keys in different files\n",
    "    for i in range(len(info)):\n",
    "        for ii in range(i+1,len(info)):\n",
    "            if (info[i].split(':')[0] + ':') in info[ii]:\n",
    "                info[ii] = info[ii].split(':')[0] + '_' + str( \"%04d\" % ii) + ':' + info[ii].split(':')[1]\n",
    "\n",
    "# for each file's data, adding filetype symbol\n",
    "    temp = ''\n",
    "    for i in ['FILED AS OF DATE','DATE AS OF CHANGE','CONFORMED PERIOD OF REPORT','ACCESSION NUMBER']:\n",
    "        for ii in range(len(info)):\n",
    "            if i in info[ii]:\n",
    "                for iii in range(ii-8,ii+2):\n",
    "                    if 'CONFORMED SUBMISSION TYPE' in info[iii]:\n",
    "                        temp = info[iii].split(':')[1]\n",
    "                info[ii] = temp + '_' + info[ii]\n",
    "            \n",
    "# transform to dictionary\n",
    "    dic = {}\n",
    "    for i in info:\n",
    "        try:\n",
    "            dic.update({i.split(':')[0]:i.split(':')[1]})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "# transform to the dataframe\n",
    "    df = pd.DataFrame(dic, index = [dic['CENTRAL INDEX KEY']])\n",
    "    \n",
    "# put the address together(street, city, state, zip)\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if 'ADDRESS' in col:\n",
    "            df[col] = df[col].str.cat(df.iloc[:,i+1:i+6],sep = ',')\n",
    "            df.iloc[:, i+1:i+6] = ''\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropduplicates_comp(infocomp):   \n",
    "    information = cleaning_comp(infocomp).T.drop_duplicates()   \n",
    "    return information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacomp = pd.DataFrame()\n",
    "companies = glob.glob('sec_edgar_filings/*')\n",
    "for company in tqdm(companies):\n",
    "    path = glob.glob(company + '/*/*.txt')\n",
    "    infocomp = get_comp(path)\n",
    "    infocomp = dropduplicates_comp(infocomp)\n",
    "    datacomp = datacomp.join(infocomp, how = 'outer')\n",
    "final_comp = datacomp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = final_comp.columns.to_list()\n",
    "done = []\n",
    "for i in range(len(colnames)):\n",
    "    counter = 1\n",
    "    if colnames[i].split('_')[0] not in done:\n",
    "        colnames[i] = colnames[i].split('_')[0]\n",
    "        for ii in range(len(colnames)):\n",
    "            if colnames[i] in colnames[ii]:\n",
    "                colnames[ii] = colnames[ii].split('_')[0] + '_' + str(counter)\n",
    "                counter += 1\n",
    "        done.append(colnames[i])\n",
    "final_comp.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comp.fillna('Null',inplace = True)\n",
    "columns = final_comp.columns.to_list()\n",
    "iterator = 2\n",
    "for i in range(final_comp.shape[0]):    \n",
    "    while (iterator < len(columns)-1):\n",
    "        if ((columns[iterator].split('_')[0] == columns[iterator-1].split('_')[0]) and (final_comp.iloc[i,iterator-1] == 'Null')):\n",
    "            final_comp.iloc[i,iterator-1] = final_comp.iloc[i,iterator]\n",
    "            final_comp.iloc[i,iterator] = 'Null'\n",
    "        iterator = iterator + final_comp.shape[0]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999\n",
    "final_comp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
